{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nfrom copy import deepcopy as dp\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\n# frequency \ndef f_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)\n\n# seed for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed_everything(seed=42)\n\n# input data dir\ninput_dir = '../input/lish-moa/'\n# upload model dataset from training kernel outputs\nmod_path1 = '../input/og-1dcnn-training-final/'                 # 1D-CNN","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mod1: 1D-CNN","metadata":{}},{"cell_type":"code","source":"SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n\nseed_everything(seed=42)\n\nsc_dic = {}\nfeat_dic = {}\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\nsample_submission = pd.read_csv(input_dir+'sample_submission.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\ntarget_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n\n# non-score targets highly correlated with scored targets will be used in pretrain\nnonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\ntmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\nmat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\nmat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\nmat_cor2.index = target_nonsc_cols\nmat_cor2.columns = target_cols\nmat_cor2 = mat_cor2.dropna()\nmat_cor2_max = mat_cor2.abs().max(axis = 1)\n\nq_n_cut = 0.9\ntarget_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\nprint(len(target_nonsc_cols2))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfeat_dic['gene'] = GENES\nfeat_dic['cell'] = CELLS\n\n# sample normalization \nq2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\nqmean = (q2+q7)/2\ntrain_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\nq2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\nqmean = (q2+q7)/2\ntest_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n\nq2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\nqmean = (q2+q7)/2\ntrain_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\nqmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\ntrain_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n\nq2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\nqmean = (q2+q7)/2\ntest_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\nqmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\ntest_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n\n# remove ctr \ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[['sig_id']+target_cols]\ntarget_ns = train[['sig_id']+target_nonsc_cols2]\n\ntrain0 = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n# drug ids\ntar_sig = target['sig_id'].tolist()\ntrain_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\ntarget = target.merge(train_drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = train_drug.drug_id.value_counts()\nvc1 = vc.loc[vc <= 19].index\nvc2 = vc.loc[vc > 19].index\n\nfeature_cols = []\nfor key_i in feat_dic.keys():\n    value_i = feat_dic[key_i]\n    print(key_i,len(value_i))\n    feature_cols += value_i\nfeature_cols0 = dp(feature_cols)\n    \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\n# Averaging on multiple SEEDS\nfor seed in SEED:\n    seed_everything(seed=seed)\n    folds = train0.copy()\n    feature_cols = dp(feature_cols0)\n    \n    # Kfold - leave drug out\n    target2 = target.copy()\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    target2['kfold'] = target2.drug_id.map(dct1)\n    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n    target2.kfold = target2.kfold.astype(int)\n\n    folds['kfold'] = target2['kfold'].copy()\n\n    train = folds.copy()\n    test_ = test.copy()\n\n    # HyperParameters\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 25\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 5\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n\n    n_comp1 = 50\n    n_comp2 = 15\n\n    num_features=len(feature_cols) + n_comp1 + n_comp2\n    num_targets=len(target_cols)\n    num_targets_0=len(target_nonsc_cols2)\n    hidden_size=4096\n\n    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n    tar_weight0_min = dp(np.min(tar_weight0))\n    tar_weight = tar_weight0_min/tar_weight0\n    np.mean(tar_weight)\n    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n    \n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                      pos_weight = pos_weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class TrainDataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n            }\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n            return dct\n\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n    class Model(nn.Module):\n\n        def __init__(self, num_features, num_targets, hidden_size):\n            super(Model, self).__init__()\n            cha_1 = 256\n            cha_2 = 512\n            cha_3 = 512\n\n            cha_1_reshape = int(hidden_size/cha_1)\n            cha_po_1 = int(hidden_size/cha_1/2)\n            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n\n            self.cha_1 = cha_1\n            self.cha_2 = cha_2\n            self.cha_3 = cha_3\n            self.cha_1_reshape = cha_1_reshape\n            self.cha_po_1 = cha_po_1\n            self.cha_po_2 = cha_po_2\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dropout1 = nn.Dropout(0.1)\n            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n            self.dropout_c1 = nn.Dropout(0.1)\n            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n\n            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2 = nn.Dropout(0.1)\n            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_1 = nn.Dropout(0.3)\n            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_2 = nn.Dropout(0.2)\n            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n\n            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n            self.flt = nn.Flatten()\n\n            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n            self.dropout3 = nn.Dropout(0.2)\n            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n        def forward(self, x):\n\n            x = self.batch_norm1(x)\n            x = self.dropout1(x)\n            x = F.celu(self.dense1(x), alpha=0.06)\n\n            x = x.reshape(x.shape[0],self.cha_1,\n                          self.cha_1_reshape)\n\n            x = self.batch_norm_c1(x)\n            x = self.dropout_c1(x)\n            x = F.relu(self.conv1(x))\n\n            x = self.ave_po_c1(x)\n\n            x = self.batch_norm_c2(x)\n            x = self.dropout_c2(x)\n            x = F.relu(self.conv2(x))\n            x_s = x\n\n            x = self.batch_norm_c2_1(x)\n            x = self.dropout_c2_1(x)\n            x = F.relu(self.conv2_1(x))\n\n            x = self.batch_norm_c2_2(x)\n            x = self.dropout_c2_2(x)\n            x = F.relu(self.conv2_2(x))\n            x =  x * x_s\n\n            x = self.max_po_c2(x)\n\n            x = self.flt(x)\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = self.dense3(x)\n\n            return x\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n\n        x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n        x_test = test_[feature_cols]\n\n        #------------ norm --------------\n        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n        col_num.sort()\n        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n\n        #------------ pca --------------\n        def pca_pre(tr,va,te,\n                    n_comp,feat_raw,feat_new):\n            pca = PCA(n_components=n_comp, random_state=42)\n            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n            return(tr2,va2,te2)\n\n\n        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n        feat_dic['pca_g'] = pca_feat_g\n        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n\n        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n        feat_dic['pca_c'] = pca_feat_g\n        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n\n        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n        \n        train_dataset = TrainDataset(x_train, y_train)\n        valid_dataset = TrainDataset(x_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        oof = np.zeros((len(train), len(target_cols)))\n        best_loss = np.inf\n\n        mod_name = mod_path1 + f\"FOLD_mod11_{seed}_{fold}_.pth\"\n        \n        model.load_state_dict(torch.load(mod_name,map_location=torch.device('cpu')))\n        model.to(DEVICE)\n        \n        oof[val_idx] = inference_fn(model, validloader, DEVICE)\n        \n        #--------------------- PREDICTION---------------------\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), len(target_cols)))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n    \n    oof_tmp = dp(oof)\n    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n\nprint(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n\ntrain0[target_cols] = oof\ntest[target_cols] = predictions\n\nsub1 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n### mod1 ###\n# train0_1 = train0.copy()\n# sub_1 = sub.copy()\n\npd.DataFrame(sc_dic,index=['sc']).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mod2 DEEP INSIGHT + RESNET","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nfrom copy import deepcopy as dp\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torchvision import transforms\nimport torchvision.models as models\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nfrom pickle import dump, load\n\nimport gc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequency \ndef f_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)\n\n# seed for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed_everything(seed=42)\n\n# input data dir\ninput_dir = '../input/lish-moa/'\n# upload model dataset from training kernel outputs\nmod_path1 = '../input/deepinsight-resnet-training-final/'          # Resnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cache_dir = \"/root/.cache/torch/hub/checkpoints/\"\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\n\n!cp ../input/resnet50/* $cache_dir\n\n!ls $cache_dir\n\nmodel = models.resnet50(pretrained = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = [0]\ninput_dir = '../input/lish-moa/'\n\nsc_dic = {}\nfeat_dic = {}\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\nsample_submission = pd.read_csv(input_dir+'sample_submission.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\ntarget_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n\n######## non-score targets highly correlated with scored targets will be used in pretrain ########\nnonctr_id = train_features.loc[:,'sig_id'].tolist()\ntmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\nmat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\nmat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\nmat_cor2.index = target_nonsc_cols\nmat_cor2.columns = target_cols\nmat_cor2 = mat_cor2.dropna()\nmat_cor2_max = mat_cor2.abs().max(axis = 1)\n\nq_n_cut = 0.9\ntarget_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\nprint(len(target_nonsc_cols2))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfeat_dic['gene'] = GENES\nfeat_dic['cell'] = CELLS\n \ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n\ntarget = train[['sig_id']+target_cols]\ntarget_ns = train[['sig_id']+target_nonsc_cols2]\n\ntrain0 = train\ntest = test_features\n\nfor df in [train0, test]:\n    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n# drug ids\ntar_sig = target['sig_id'].tolist()\ntrain_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\ntarget = target.merge(train_drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = train_drug.drug_id.value_counts()\nvc1 = vc.loc[vc <= 19].index\nvc2 = vc.loc[vc > 19].index\n\nfeature_cols = []\nfor key_i in feat_dic.keys():\n    value_i = feat_dic[key_i]\n    print(key_i,len(value_i))\n    feature_cols += value_i\nlen(feature_cols)\nfeature_cols0 = dp(feature_cols)\n    \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for seed in SEED:\n    seed_everything(seed=seed)\n    folds = train0.copy()\n    feature_cols = dp(feature_cols0)\n    \n    # Kfold - leave drug out\n    target2 = target.copy()\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    target2['kfold'] = target2.drug_id.map(dct1)\n    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n    target2.kfold = target2.kfold.astype(int)\n\n    folds['kfold'] = target2['kfold'].copy()\n\n    train = folds.copy()\n    test_ = test.copy()\n\n    # HyperParameters\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 20\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 5\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n\n    n_comp1 = 50\n    n_comp2 = 15\n\n    num_features=len(feature_cols) + n_comp1 + n_comp2\n    num_targets=len(target_cols)\n    num_targets_0=len(target_nonsc_cols2)\n    hidden_size=4096\n\n    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n    tar_weight0_min = dp(np.min(tar_weight0))\n    tar_weight = tar_weight0_min/tar_weight0\n    np.mean(tar_weight)\n    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n    \n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                      pos_weight = pos_weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n        \n    # Alicia - DeepInsight\n    class LogScaler:\n        \"\"\"Log normalize and scale data\n\n        Log normalization and scaling procedure as described as norm-2 in the\n        DeepInsight paper supplementary information.\n\n        Note: The dimensions of input matrix is (N samples, d features)\n        \"\"\"\n        def __init__(self):\n            self._min0 = None\n            self._max = None\n\n        \"\"\"\n        Use this as a preprocessing step in inference mode.\n        \"\"\"\n        def fit(self, X, y=None):\n            # Min. of training set per feature\n            self._min0 = X.min(axis=0)\n\n            # Log normalized X by log(X + _min0 + 1)\n            X_norm = np.log(X + np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) + 1).clip(min=0, max=None)\n            # Global max. of training set from X_norm\n            self._max = X_norm.max()\n\n        \"\"\"\n        For training set only.\n        \"\"\"\n        def fit_transform(self, X, y=None):\n            # Min. of training set per feature\n            self._min0 = X.min(axis=0)\n\n            # Log normalized X by log(X + _min0 + 1)\n            X_norm = np.log(X + np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) + 1).clip(min=0, max=None)\n\n            # Global max. of training set from X_norm\n            self._max = X_norm.max()\n\n            # Normalized again by global max. of training set\n            return (X_norm / self._max).clip(0, 1)\n\n        \"\"\"\n        For validation and test set only.\n        \"\"\"\n        def transform(self, X, y=None): \n            # Adjust min. of each feature of X by _min0\n            for i in range(X.shape[1]):\n                X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n\n            # Log normalized X by log(X + _min0 + 1)\n            X_norm = np.log(\n                X +\n                np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n                1).clip(min=0, max=None)\n\n            # Normalized again by global max. of training set\n            return (X_norm / self._max).clip(0, 1)\n    \n    class DeepInsightTransformer:\n        \"\"\"Transform features to an image matrix using dimensionality reduction\n\n        This class takes in data normalized between 0 and 1 and converts it to a\n        CNN compatible 'image' matrix\n\n        \"\"\"\n        def __init__(self,\n                     feature_extractor='tsne',\n                     perplexity=30,\n                     pixels=100,\n                     random_state=None,\n                     n_jobs=None):\n            \"\"\"Generate an ImageTransformer instance\n\n            Args:\n                feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n                    class instance with method `fit_transform` that returns a\n                    2-dimensional array of extracted features.\n                pixels: int (square matrix) or tuple of ints (height, width) that\n                    defines the size of the image matrix.\n                random_state: int or RandomState. Determines the random number\n                    generator, if present, of a string defined feature_extractor.\n                n_jobs: The number of parallel jobs to run for a string defined\n                    feature_extractor.\n            \"\"\"\n            self.random_state = random_state\n            self.n_jobs = n_jobs\n\n            if isinstance(feature_extractor, str):\n                fe = feature_extractor.casefold()\n                if fe == 'tsne_exact'.casefold():\n                    fe = TSNE(n_components=2,\n                              metric='cosine',\n                              perplexity=perplexity,\n                              n_iter=1000,\n                              method='exact',\n                              random_state=self.random_state,\n                              n_jobs=self.n_jobs)\n                elif fe == 'tsne'.casefold():\n                    fe = TSNE(n_components=2,\n                              metric='cosine',\n                              perplexity=perplexity,\n                              n_iter=1000,\n                              method='barnes_hut',\n                              random_state=self.random_state,\n                              n_jobs=self.n_jobs)\n                elif fe == 'pca'.casefold():\n                    fe = PCA(n_components=2, random_state=self.random_state)\n                elif fe == 'kpca'.casefold():\n                    fe = KernelPCA(n_components=2,\n                                   kernel='rbf',\n                                   random_state=self.random_state,\n                                   n_jobs=self.n_jobs)\n                else:\n                    raise ValueError((\"Feature extraction method '{}' not accepted\"\n                                      ).format(feature_extractor))\n                self._fe = fe\n            elif hasattr(feature_extractor, 'fit_transform') and \\\n                    inspect.ismethod(feature_extractor.fit_transform):\n                self._fe = feature_extractor\n            else:\n                raise TypeError('Parameter feature_extractor is not a '\n                                'string nor has method \"fit_transform\"')\n\n            if isinstance(pixels, int):\n                pixels = (pixels, pixels)\n\n            # The resolution of transformed image\n            self._pixels = pixels\n            self._xrot = None\n\n        def fit(self, X, y=None, plot=False):\n            \"\"\"Train the image transformer from the training set (X)\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                y: Ignored. Present for continuity with scikit-learn\n                plot: boolean of whether to produce a scatter plot showing the\n                    feature reduction, hull points, and minimum bounding rectangle\n\n            Returns:\n                self: object\n            \"\"\"\n            # Transpose to get (n_features, n_samples)\n            X = X.T\n\n            # Perform dimensionality reduction\n            x_new = self._fe.fit_transform(X)\n\n            # Get the convex hull for the points\n            chvertices = ConvexHull(x_new).vertices\n            hull_points = x_new[chvertices]\n\n            # Determine the minimum bounding rectangle\n            mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n\n            # Rotate the matrix\n            # Save the rotated matrix in case user wants to change the pixel size\n            self._xrot = np.dot(mbr_rot, x_new.T).T\n\n            # Determine feature coordinates based on pixel dimension\n            self._calculate_coords()\n\n            # plot rotation diagram if requested\n            if plot is True:\n                # Create subplots\n                fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n                ax[0, 0].scatter(x_new[:, 0],\n                                 x_new[:, 1],\n                                 cmap=plt.cm.get_cmap(\"jet\", 10),\n                                 marker=\"x\",\n                                 alpha=1.0)\n                ax[0, 0].fill(x_new[chvertices, 0],\n                              x_new[chvertices, 1],\n                              edgecolor='r',\n                              fill=False)\n                ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n                plt.gca().set_aspect('equal', adjustable='box')\n                plt.show()\n            return self\n\n        @property\n        def pixels(self):\n            \"\"\"The image matrix dimensions\n\n            Returns:\n                tuple: the image matrix dimensions (height, width)\n\n            \"\"\"\n            return self._pixels\n\n        @pixels.setter\n        def pixels(self, pixels):\n            \"\"\"Set the image matrix dimension\n\n            Args:\n                pixels: int or tuple with the dimensions (height, width)\n                of the image matrix\n\n            \"\"\"\n            if isinstance(pixels, int):\n                pixels = (pixels, pixels)\n            self._pixels = pixels\n            # recalculate coordinates if already fit\n            if hasattr(self, '_coords'):\n                self._calculate_coords()\n\n        def _calculate_coords(self):\n            \"\"\"Calculate the matrix coordinates of each feature based on the\n            pixel dimensions.\n            \"\"\"\n            ax0_coord = np.digitize(self._xrot[:, 0],\n                                    bins=np.linspace(min(self._xrot[:, 0]),\n                                                     max(self._xrot[:, 0]),\n                                                     self._pixels[0])) - 1\n            ax1_coord = np.digitize(self._xrot[:, 1],\n                                    bins=np.linspace(min(self._xrot[:, 1]),\n                                                     max(self._xrot[:, 1]),\n                                                     self._pixels[1])) - 1\n            self._coords = np.stack((ax0_coord, ax1_coord))\n\n        def transform(self, X, empty_value=0):\n            \"\"\"Transform the input matrix into image matrices\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                    where n_features matches the training set.\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n            # Group by location (x1, y1) of each feature\n            # Tranpose to get (n_features, n_samples)\n            img_coords = pd.DataFrame(np.vstack(\n                (self._coords, X.clip(0, 1))).T).groupby(\n                    [0, 1],  # (x1, y1)\n                    as_index=False).mean()\n\n            img_matrices = []\n            blank_mat = np.zeros(self._pixels)\n            if empty_value != 0:\n                blank_mat[:] = empty_value\n            for z in range(2, img_coords.shape[1]):\n                img_matrix = blank_mat.copy()\n                img_matrix[img_coords[0].astype(int),\n                           img_coords[1].astype(int)] = img_coords[z]\n                img_matrices.append(img_matrix)\n                \n            img_matrices = np.array([self._mat_to_rgb(m) for m in img_matrices])\n                \n            return img_matrices\n        \n        def transform_3d(self, X, empty_value=0):\n            \"\"\"Transform the input matrix into image matrices\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                    where n_features matches the training set.\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n\n            # Group by location (x1, y1) of each feature\n            # Tranpose to get (n_features, n_samples)\n            img_coords = pd.DataFrame(np.vstack(\n                (self._coords, X.clip(0, 1))).T).groupby(\n                    [0, 1],  # (x1, y1)\n                    as_index=False)\n            avg_img_coords = img_coords.mean()\n            min_img_coords = img_coords.min()\n            max_img_coords = img_coords.max()\n\n            img_matrices = []\n            blank_mat = np.zeros((3, self._pixels[0], self._pixels[1]))\n            if empty_value != 0:\n                blank_mat[:, :, :] = empty_value\n            for z in range(2, avg_img_coords.shape[1]):\n                img_matrix = blank_mat.copy()\n                img_matrix[0, avg_img_coords[0].astype(int),\n                           avg_img_coords[1].astype(int)] = avg_img_coords[z]\n                img_matrix[1, min_img_coords[0].astype(int),\n                           min_img_coords[1].astype(int)] = min_img_coords[z]\n                img_matrix[2, max_img_coords[0].astype(int),\n                           max_img_coords[1].astype(int)] = max_img_coords[z]\n                img_matrices.append(img_matrix)\n\n            return img_matrices\n\n        def fit_transform(self, X, empty_value=0):\n            \"\"\"Train the image transformer from the training set (X) and return\n            the transformed data.\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n            self.fit(X)\n            return self.transform(X, empty_value=empty_value)\n        \n        def fit_transform_3d(self, X, empty_value=0):\n            \"\"\"Train the image transformer from the training set (X) and return\n            the transformed data.\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n            self.fit(X)\n            return self.transform_3d(X, empty_value=empty_value)\n\n        def feature_density_matrix(self):\n            \"\"\"Generate image matrix with feature counts per pixel\n\n            Returns:\n                img_matrix (ndarray): matrix with feature counts per pixel\n            \"\"\"\n            fdmat = np.zeros(self._pixels)\n            # Group by location (x1, y1) of each feature\n            # Tranpose to get (n_features, n_samples)\n            coord_cnt = (\n                pd.DataFrame(self._coords.T).assign(count=1).groupby(\n                    [0, 1],  # (x1, y1)\n                    as_index=False).count())\n            fdmat[coord_cnt[0].astype(int),\n                  coord_cnt[1].astype(int)] = coord_cnt['count']\n            return fdmat\n\n        @staticmethod\n        def _minimum_bounding_rectangle(hull_points):\n            \"\"\"Find the smallest bounding rectangle for a set of points.\n\n            Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n            Returns a set of points representing the corners of the bounding box.\n\n            Args:\n                hull_points : an nx2 matrix of hull coordinates\n\n            Returns:\n                (tuple): tuple containing\n                    coords (ndarray): coordinates of the corners of the rectangle\n                    rotmat (ndarray): rotation matrix to align edges of rectangle\n                        to x and y\n            \"\"\"\n\n            pi2 = np.pi / 2.\n\n            # Calculate edge angles\n            edges = hull_points[1:] - hull_points[:-1]\n            angles = np.arctan2(edges[:, 1], edges[:, 0])\n            angles = np.abs(np.mod(angles, pi2))\n            angles = np.unique(angles)\n\n            # Find rotation matrices\n            rotations = np.vstack([\n                np.cos(angles),\n                np.cos(angles - pi2),\n                np.cos(angles + pi2),\n                np.cos(angles)\n            ]).T\n            rotations = rotations.reshape((-1, 2, 2))\n\n            # Apply rotations to the hull\n            rot_points = np.dot(rotations, hull_points.T)\n\n            # Find the bounding points\n            min_x = np.nanmin(rot_points[:, 0], axis=1)\n            max_x = np.nanmax(rot_points[:, 0], axis=1)\n            min_y = np.nanmin(rot_points[:, 1], axis=1)\n            max_y = np.nanmax(rot_points[:, 1], axis=1)\n\n            # Find the box with the best area\n            areas = (max_x - min_x) * (max_y - min_y)\n            best_idx = np.argmin(areas)\n\n            # Return the best box\n            x1 = max_x[best_idx]\n            x2 = min_x[best_idx]\n            y1 = max_y[best_idx]\n            y2 = min_y[best_idx]\n            rotmat = rotations[best_idx]\n\n            # Generate coordinates\n            coords = np.zeros((4, 2))\n            coords[0] = np.dot([x1, y2], rotmat)\n            coords[1] = np.dot([x2, y2], rotmat)\n            coords[2] = np.dot([x2, y1], rotmat)\n            coords[3] = np.dot([x1, y1], rotmat)\n\n            return coords, rotmat\n        \n        @staticmethod\n        def _mat_to_rgb(mat):\n            \"\"\"Convert image matrix to numpy rgb format\n            Args:\n                mat: {array-like} (M, N)\n            Returns:\n                An numpy.ndarry (M, N, 3) with orignal values repeated across\n                RGB channels.\n            \"\"\"\n            return np.repeat(mat[:, :, np.newaxis], 3, axis=2)\n\n    class TrainDataset:\n        def __init__(self, features, labels, transformer):\n            self.features = features\n            self.labels = labels\n            self.transformer = transformer\n\n        def __getitem__(self, index):\n            normalized = self.features[index, :]\n            normalized = np.expand_dims(normalized, axis=0)\n\n            # Note: we are setting empty_value=1 to follow the setup in the paper\n            image = self.transformer.transform(normalized, empty_value=1)[0]\n            \n            preprocess = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n\n            return {\"x\": preprocess(image).type(torch.float), \"y\": torch.tensor(self.labels[index, :],dtype=torch.float)}\n        \n        \n        def __len__(self):\n            return self.features.shape[0]\n\n    class TestDataset:\n        def __init__(self, features, transformer):\n            self.features = features\n            self.transformer = transformer\n\n        def __getitem__(self, index):\n            normalized = self.features[index, :]\n            normalized = np.expand_dims(normalized, axis=0)\n\n            # Note: we are setting empty_value=1 to follow the setup in the paper\n            image = self.transformer.transform(normalized, empty_value=1)[0]\n            \n            preprocess = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n            \n            return  {\"x\": preprocess(image).type(torch.float)}\n\n        def __len__(self):\n            return self.features.shape[0]\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n\n        x_train, y_train,y_train_ns = train_df[feature_cols].values, train_df[target_cols].values,train_df[target_nonsc_cols2].values\n        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols].values, valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n        x_test = test_[feature_cols].values\n        \n        def load_pickle(model_output_folder, seed_i, fold_i, name):\n            return load(open(f\"{model_output_folder}/seed{seed_i}_fold{fold_i}_{name}.pkl\", 'rb'))\n        \n        # LogScaler (Norm-2 Normalization)\n        \n        scaler = load_pickle(mod_path1 + \"Resnet_deepinsight\", seed, fold, \"log-scaler\")\n        x_test = scaler.transform(x_test)\n        x_train = scaler.transform(x_train)\n        x_valid = scaler.transform(x_valid)\n        \n        # Load DeepInsight Feature Map\n        transformer = load_pickle(mod_path1 + \"Resnet_deepinsight\", seed, fold,\n                                  \"deepinsight-transform\")\n        \n        \n        train_dataset = TrainDataset(x_train, y_train, transformer)\n        valid_dataset = TrainDataset(x_valid, y_valid, transformer)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        oof = np.zeros((len(train), len(target_cols)))\n        best_loss = np.inf\n\n        mod_name = mod_path1 + f\"FOLD_mod11_{seed}_{fold}_.pth\"\n        \n        model.fc = nn.Sequential(nn.Linear(2048, 512),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.2),\n                                 nn.Linear(512, num_targets))\n        \n        model.load_state_dict(torch.load(mod_name))\n        model.to(DEVICE)\n        \n        oof[val_idx] = inference_fn(model, validloader, DEVICE)\n        \n        #--------------------- PREDICTION---------------------\n        testdataset = TestDataset(x_test, transformer)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), len(target_cols)))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            print('seed:',seed,'fold:',fold)\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n    \n    oof_tmp = dp(oof)\n    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n\nprint(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n\ntrain0[target_cols] = oof\ntest[target_cols] = predictions\n\nsub2 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n### mod2 ###\ntrain0_1 = train0.copy()\nsub_1 = sub2.copy()\n\npd.DataFrame(sc_dic,index=['sc']).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final submission\nsub2.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ENSEMBLE","metadata":{"trusted":true}},{"cell_type":"code","source":"submission = pd.read_csv(input_dir+'sample_submission.csv')\nsubmission.iloc[:, 1:] = 0\nsubmission.iloc[:, 1:] = sub1.iloc[:,1:]*0.9 + sub2.iloc[:,1:]*0.025 \nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}