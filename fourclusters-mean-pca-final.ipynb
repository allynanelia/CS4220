{"metadata":{"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\n\nfrom statistics import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed) \n    tf.random.set_seed(seed)\n\nseed_everything(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Loading dataframes","metadata":{}},{"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrain_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\ngene_cluster = pd.read_csv('/kaggle/input/list-moa/gene_cluster.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data processing","metadata":{}},{"cell_type":"code","source":"# Convert categorical variable into dummy/indicator variables\ntrain_features = pd.get_dummies(train_features, columns=['cp_type', 'cp_dose'])\ntest_features = pd.get_dummies(test_features, columns=['cp_type', 'cp_dose'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove sig_id\nall_dfs = (train_features, train_targets, test_features)\n\nfor df in all_dfs:\n    if 'sig_id' in df.columns:\n        df.drop('sig_id', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gene clustering\nGENE_COLS = [col for col in train_features.columns if col.startswith('g-')]\n\ngroup1 = gene_cluster['cluster'].isin(['1'])\ngroup2 = gene_cluster['cluster'].isin(['2'])\ngroup3 = gene_cluster['cluster'].isin(['3'])\ngroup4 = gene_cluster['cluster'].isin(['4'])\n\ngroup1_indices = np.where(group1)[0]\ngroup2_indices = np.where(group2)[0]\ngroup3_indices = np.where(group3)[0]\ngroup4_indices = np.where(group4)[0]\n\nGENE_COLS_1 = [GENE_COLS[i] for i in group1_indices]\nGENE_COLS_2 = [GENE_COLS[i] for i in group2_indices]\nGENE_COLS_3 = [GENE_COLS[i] for i in group3_indices]\nGENE_COLS_4 = [GENE_COLS[i] for i in group4_indices]\n\ntrainrows_list = np.arange(0, train_features.shape[0], 1).tolist()\ntrain_features['cluster1'] = [mean(list(train_features.loc[i,GENE_COLS_1])) for i in trainrows_list]\ntrain_features['cluster2'] = [mean(list(train_features.loc[i,GENE_COLS_2])) for i in trainrows_list]\ntrain_features['cluster3'] = [mean(list(train_features.loc[i,GENE_COLS_3])) for i in trainrows_list]\ntrain_features['cluster4'] = [mean(list(train_features.loc[i,GENE_COLS_4])) for i in trainrows_list]\n\ntestrows_list = np.arange(0, test_features.shape[0], 1).tolist()\ntest_features['cluster1'] = [mean(list(test_features.loc[i,GENE_COLS_1])) for i in testrows_list]\ntest_features['cluster2'] = [mean(list(test_features.loc[i,GENE_COLS_2])) for i in testrows_list]\ntest_features['cluster3'] = [mean(list(test_features.loc[i,GENE_COLS_3])) for i in testrows_list]\ntest_features['cluster4'] = [mean(list(test_features.loc[i,GENE_COLS_4])) for i in testrows_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. PCA","metadata":{}},{"cell_type":"code","source":"CELL_COLS = [col for col in train_features.columns if col.startswith('c-')]\n\ntrain_features_gene = train_features.loc[:, GENE_COLS]\ntest_features_gene = test_features.loc[:, GENE_COLS]\ntrain_features_cell = train_features.loc[:, CELL_COLS]\ntest_features_cell = test_features.loc[:, CELL_COLS]\n\nN_COMP_GENE = 30\nN_COMP_CELL = 10\n\n# # PCA for gene\npca_gene_train = PCA(n_components=N_COMP_GENE).fit_transform(train_features_gene)\npca_gene_train = pd.DataFrame(data=pca_gene_train, columns=[f'pc-g-{i}' for i in range(N_COMP_GENE)])\npca_gene_test = PCA(n_components=N_COMP_GENE).fit_transform(test_features_gene)\npca_gene_test = pd.DataFrame(data=pca_gene_test, columns=[f'pc-g-{i}' for i in range(N_COMP_GENE)])\n\n# # PCA for cell\npca_cell_train = PCA(n_components=N_COMP_CELL).fit_transform(train_features_cell)\npca_cell_train = pd.DataFrame(data=pca_cell_train, columns=[f'pc-c-{i}' for i in range(N_COMP_CELL)])\npca_cell_test = PCA(n_components=N_COMP_CELL).fit_transform(test_features_cell)\npca_cell_test = pd.DataFrame(data=pca_cell_test, columns=[f'pc-c-{i}' for i in range(N_COMP_CELL)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Appending new components to existing features\npca_train = train_features.copy()\npca_train = pd.concat([pca_train, pca_gene_train, pca_cell_train], axis=1)\n\npca_test = test_features.copy()\npca_test= pd.concat([pca_test, pca_gene_test, pca_cell_test], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Model-building function","metadata":{}},{"cell_type":"code","source":"def get_model(train_df, *units):\n    model = tf.keras.Sequential()\n    \n    model.add(tf.keras.layers.Input(shape=(train_df.shape[1],)))\n    model.add(tf.keras.layers.BatchNormalization())\n    \n    for u in units[:-1]:\n        model.add(tf.keras.layers.Dense(units=u, activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.55))\n    \n    model.add(tf.keras.layers.Dense(units=units[-1], activation=\"sigmoid\"))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Preparing datasets and stuff","metadata":{}},{"cell_type":"code","source":"### MODIFY THIS FOR DIFFERENT DATASETS ###\n\nTHE_TRAIN = pca_train\nTHE_TEST = pca_test\n\n##########################################\n\npred_val = np.zeros((THE_TRAIN.shape[0], 206))\npred_test = np.zeros((THE_TEST.shape[0], 206))\n\nfeatures = THE_TRAIN.values\ntargets = train_targets.values\ntests = THE_TEST.values\n\nvalidation_scores = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Run model","metadata":{}},{"cell_type":"code","source":"def run(features, targets, tests, pred, pe, n_split=5):\n    kfoldnumber = 0\n\n    for train_index, validation_index in KFold(n_split).split(features):\n        kfoldnumber += 1\n        print(f'{\"#\" * 30} Fold number {kfoldnumber} {\"#\" * 30}')\n\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',\n                                           factor=0.1, patience=3, verbose=0,\n                                           epsilon=1e-4, mode='min')\n        \n        nn_layers = (512, 1024, 206)\n        model = get_model(features, *nn_layers)\n        #model.summary()\n\n        model.fit(features[train_index],\n                  targets[train_index],\n                  batch_size=128,\n                  epochs=35,\n                  validation_data=(features[validation_index], targets[validation_index]),\n                  verbose=0,\n                  callbacks=[reduce_lr_loss])\n\n        print()\n        print('train loss:\\t', model.evaluate(features[train_index], targets[train_index],\n                                     verbose=0, batch_size=128))\n        \n        validate_score = model.evaluate(features[validation_index],\n                                         targets[validation_index],\n                                        verbose=0, batch_size=128)\n        print('validate loss:\\t', validate_score)\n        validation_scores.append(validate_score)\n        \n\n        print()\n        print('predict validation...')\n\n        pred[validation_index] = model.predict(features[validation_index],\n                                              verbose=0, batch_size=128)\n\n        print('predict test...')\n\n        pe += model.predict(tests, verbose=0, batch_size=128) / n_split\n        print()\n    \n    print('###########################################################################\\n\\nFIN')\n    return pred, pe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run model\npred_val, pred_test = run(features, targets, tests, pred_val, pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average prediction validation score\nprint(sum(validation_scores)/(len(validation_scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Submission","metadata":{}},{"cell_type":"code","source":"moa_dir = '../input/lish-moa/'\ncolumns = pd.read_csv(moa_dir + \"train_targets_scored.csv\")\ncolumns.drop('sig_id', axis=1, inplace=True)\nsubmission = pd.DataFrame(data=pred_test, columns=columns.columns)\nsample = pd.read_csv(moa_dir + \"sample_submission.csv\")\nsubmission.insert(0, column='sig_id', value=sample['sig_id'])\n\nsubmission.to_csv('submission.csv', index=False)\nprint(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}