{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nfrom copy import deepcopy as dp\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torchvision import transforms\nimport torchvision.models as models\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nfrom pickle import dump, load\n\nimport gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NAIVE MODEL","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed) \n    tf.random.set_seed(seed)\n\nseed_everything(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dir = '../input/lish-moa/'\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets = pd.read_csv(input_dir+'train_targets_scored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert categorical variable into dummy/indicator variables\ntrain_features = pd.get_dummies(train_features, columns=['cp_type', 'cp_dose'])\ntest_features = pd.get_dummies(test_features, columns=['cp_type', 'cp_dose'])\n\n# Remove sig_id\nall_dfs = (train_features, train_targets, test_features)\n\nfor df in all_dfs:\n    if 'sig_id' in df.columns:\n        df.drop('sig_id', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GENE_COLS = [col for col in train_features.columns if col.startswith('g-')]\nCELL_COLS = [col for col in train_features.columns if col.startswith('c-')]\n\ntrain_features_gene = train_features.loc[:, GENE_COLS]\ntest_features_gene = test_features.loc[:, GENE_COLS]\ntrain_features_cell = train_features.loc[:, CELL_COLS]\ntest_features_cell = test_features.loc[:, CELL_COLS]\n\nN_COMP_GENE = 90\nN_COMP_CELL = 50\n\n# PCA for gene\npca_gene_train = PCA(n_components=N_COMP_GENE).fit_transform(train_features_gene)\npca_gene_train = pd.DataFrame(data=pca_gene_train, columns=[f'pc-g-{i}' for i in range(N_COMP_GENE)])\npca_gene_test = PCA(n_components=N_COMP_GENE).fit_transform(test_features_gene)\npca_gene_test = pd.DataFrame(data=pca_gene_test, columns=[f'pc-g-{i}' for i in range(N_COMP_GENE)])\n\n# PCA for cell\npca_cell_train = PCA(n_components=N_COMP_CELL).fit_transform(train_features_cell)\npca_cell_train = pd.DataFrame(data=pca_cell_train, columns=[f'pc-c-{i}' for i in range(N_COMP_CELL)])\npca_cell_test = PCA(n_components=N_COMP_CELL).fit_transform(test_features_cell)\npca_cell_test = pd.DataFrame(data=pca_cell_test, columns=[f'pc-c-{i}' for i in range(N_COMP_CELL)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appending new components to existing features\npca_train = train_features.copy()\n# pca_train = pd.concat([pca_train, pca_gene_train, pca_cell_train], axis=1)\n\npca_test = test_features.copy()\n# pca_test = pd.concat([pca_test, pca_gene_test, pca_cell_test], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\npca = PCA().fit(train_features_cell)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### MODIFY THIS FOR DIFFERENT DATASETS ###\n\nTHE_TRAIN = pca_train\nTHE_TEST = pca_test\n\n##########################################\n\npred_val = np.zeros((THE_TRAIN.shape[0], 206))\npred_test = np.zeros((THE_TEST.shape[0], 206))\n\nfeatures = THE_TRAIN.values\ntargets = train_targets.values\ntests = THE_TEST.values\n\nvalidation_scores = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(train_df, *units):\n    model = tf.keras.Sequential()\n    \n    model.add(tf.keras.layers.Input(shape=(train_df.shape[1],)))\n    model.add(tf.keras.layers.BatchNormalization())\n    \n    for u in units[:-1]:\n        model.add(tf.keras.layers.Dense(units=u, activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(tf.keras.layers.Dense(units=units[-1], activation=\"sigmoid\"))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model\n\ndef run(features, targets, tests, pred, pe, n_split=5):\n    kfoldnumber = 0\n\n    for train_index, validation_index in KFold(n_split).split(features):\n        kfoldnumber += 1\n        print(f'{\"#\" * 30} Fold number {kfoldnumber} {\"#\" * 30}')\n\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',\n                                           factor=0.1, patience=3, verbose=0,\n                                           epsilon=1e-4, mode='min')\n        \n        nn_layers = (512, 1024, 206)\n        model = get_model(features, *nn_layers)\n        #model.summary()\n\n        model.fit(features[train_index],\n                  targets[train_index],\n                  batch_size=128,\n                  epochs=35,\n                  validation_data=(features[validation_index], targets[validation_index]),\n                  verbose=0,\n                  callbacks=[reduce_lr_loss])\n\n        print()\n        print('train loss:\\t', model.evaluate(features[train_index], targets[train_index],\n                                     verbose=0, batch_size=128))\n        \n        validate_score = model.evaluate(features[validation_index],\n                                         targets[validation_index],\n                                        verbose=0, batch_size=128)\n        print('validate loss:\\t', validate_score)\n        validation_scores.append(validate_score)\n        \n\n        print()\n        print('predict validation...')\n\n        pred[validation_index] = model.predict(features[validation_index],\n                                              verbose=0, batch_size=128)\n\n        print('predict test...')\n\n        pe += model.predict(tests, verbose=0, batch_size=128) / n_split\n        print()\n        \n        del model    \n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    print('###########################################################################\\n\\nFIN')\n    \n    return pred, pe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run model\npred_val, pred_test = run(features, targets, tests, pred_val, pred_test)\n\n# Average prediction validation score\nprint(sum(validation_scores)/(len(validation_scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = pd.read_csv(input_dir + \"train_targets_scored.csv\")\ncolumns.drop('sig_id', axis=1, inplace=True)\nsub1 = pd.DataFrame(data=pred_test, columns=columns.columns)\nsample = pd.read_csv(input_dir + \"sample_submission.csv\")\nsub1.insert(0, column='sig_id', value=sample['sig_id'])\n\nsub1.to_csv('submission.csv', index=False)\nprint(sub1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DEEP INSIGHT + RESNET\n","metadata":{}},{"cell_type":"code","source":"# # TabNet\n# !pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n\n# # Tabnet \n# from torch.optim.lr_scheduler import ReduceLROnPlateau\n# from pytorch_tabnet.metrics import Metric\n# from pytorch_tabnet.tab_model import TabNetRegressor\n\n# feature transformation - fit\ndef norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n    ss_1_dic = {'zsco':StandardScaler(),\n                'mima':MinMaxScaler(),\n                'maxb':MaxAbsScaler(), \n                'robu':RobustScaler(),\n                'norm':Normalizer(), \n                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n                'powe':PowerTransformer()}\n    ss_1 = ss_1_dic[sc_name]\n    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n    if saveM == False:\n        return(df_2)\n    else:\n        return(df_2,ss_1)\n\n# feature transformation - trans\ndef norm_tra(df_1,ss_x):\n    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n    return(df_2)\n\n# frequency \ndef f_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)\n\n# seed for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed_everything(seed=42)\n\n# input data dir\ninput_dir = '../input/lish-moa/'\n# upload model dataset from training kernel outputs\nmod_path1 = '../input/deepinsight-resnet-training-final/deepinsight-resnet-training/'          # Resnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cache_dir = \"/root/.cache/torch/hub/checkpoints/\"\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\n\n!cp ../input/resnet50/* $cache_dir\n\n!ls $cache_dir\n\nmodel = models.resnet50(pretrained = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = [0]\ninput_dir = '../input/lish-moa/'\n\nsc_dic = {}\nfeat_dic = {}\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\nsample_submission = pd.read_csv(input_dir+'sample_submission.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\ntarget_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n\n######## non-score targets highly correlated with scored targets will be used in pretrain ########\nnonctr_id = train_features.loc[:,'sig_id'].tolist()\ntmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\nmat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\nmat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\nmat_cor2.index = target_nonsc_cols\nmat_cor2.columns = target_cols\nmat_cor2 = mat_cor2.dropna()\nmat_cor2_max = mat_cor2.abs().max(axis = 1)\n\nq_n_cut = 0.9\ntarget_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\nprint(len(target_nonsc_cols2))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfeat_dic['gene'] = GENES\nfeat_dic['cell'] = CELLS\n \ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n\ntarget = train[['sig_id']+target_cols]\ntarget_ns = train[['sig_id']+target_nonsc_cols2]\n\ntrain0 = train\ntest = test_features\n\nfor df in [train0, test]:\n    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n# drug ids\ntar_sig = target['sig_id'].tolist()\ntrain_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\ntarget = target.merge(train_drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = train_drug.drug_id.value_counts()\nvc1 = vc.loc[vc <= 19].index\nvc2 = vc.loc[vc > 19].index\n\nfeature_cols = []\nfor key_i in feat_dic.keys():\n    value_i = feat_dic[key_i]\n    print(key_i,len(value_i))\n    feature_cols += value_i\nlen(feature_cols)\nfeature_cols0 = dp(feature_cols)\n    \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for seed in SEED:\n    seed_everything(seed=seed)\n    folds = train0.copy()\n    feature_cols = dp(feature_cols0)\n    \n    # Kfold - leave drug out\n    target2 = target.copy()\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    target2['kfold'] = target2.drug_id.map(dct1)\n    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n    target2.kfold = target2.kfold.astype(int)\n\n    folds['kfold'] = target2['kfold'].copy()\n\n    train = folds.copy()\n    test_ = test.copy()\n\n    # HyperParameters\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 20\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 5\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n\n    n_comp1 = 50\n    n_comp2 = 15\n\n    num_features=len(feature_cols) + n_comp1 + n_comp2\n    num_targets=len(target_cols)\n    num_targets_0=len(target_nonsc_cols2)\n    hidden_size=4096\n\n    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n    tar_weight0_min = dp(np.min(tar_weight0))\n    tar_weight = tar_weight0_min/tar_weight0\n    np.mean(tar_weight)\n    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n    \n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                      pos_weight = pos_weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n        \n    # Alicia - DeepInsight\n    class LogScaler:\n        \"\"\"Log normalize and scale data\n\n        Log normalization and scaling procedure as described as norm-2 in the\n        DeepInsight paper supplementary information.\n\n        Note: The dimensions of input matrix is (N samples, d features)\n        \"\"\"\n        def __init__(self):\n            self._min0 = None\n            self._max = None\n\n        \"\"\"\n        Use this as a preprocessing step in inference mode.\n        \"\"\"\n        def fit(self, X, y=None):\n            # Min. of training set per feature\n            self._min0 = X.min(axis=0)\n\n            # Log normalized X by log(X + _min0 + 1)\n            X_norm = np.log(X + np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) + 1).clip(min=0, max=None)\n            # Global max. of training set from X_norm\n            self._max = X_norm.max()\n\n        \"\"\"\n        For training set only.\n        \"\"\"\n        def fit_transform(self, X, y=None):\n            # Min. of training set per feature\n            self._min0 = X.min(axis=0)\n\n            # Log normalized X by log(X + _min0 + 1)\n            X_norm = np.log(X + np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) + 1).clip(min=0, max=None)\n\n            # Global max. of training set from X_norm\n            self._max = X_norm.max()\n\n            # Normalized again by global max. of training set\n            return (X_norm / self._max).clip(0, 1)\n\n        \"\"\"\n        For validation and test set only.\n        \"\"\"\n        def transform(self, X, y=None): \n            # Adjust min. of each feature of X by _min0\n            for i in range(X.shape[1]):\n                X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n\n            # Log normalized X by log(X + _min0 + 1)\n            X_norm = np.log(\n                X +\n                np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n                1).clip(min=0, max=None)\n\n            # Normalized again by global max. of training set\n            return (X_norm / self._max).clip(0, 1)\n    \n    class DeepInsightTransformer:\n        \"\"\"Transform features to an image matrix using dimensionality reduction\n\n        This class takes in data normalized between 0 and 1 and converts it to a\n        CNN compatible 'image' matrix\n\n        \"\"\"\n        def __init__(self,\n                     feature_extractor='tsne',\n                     perplexity=30,\n                     pixels=100,\n                     random_state=None,\n                     n_jobs=None):\n            \"\"\"Generate an ImageTransformer instance\n\n            Args:\n                feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n                    class instance with method `fit_transform` that returns a\n                    2-dimensional array of extracted features.\n                pixels: int (square matrix) or tuple of ints (height, width) that\n                    defines the size of the image matrix.\n                random_state: int or RandomState. Determines the random number\n                    generator, if present, of a string defined feature_extractor.\n                n_jobs: The number of parallel jobs to run for a string defined\n                    feature_extractor.\n            \"\"\"\n            self.random_state = random_state\n            self.n_jobs = n_jobs\n\n            if isinstance(feature_extractor, str):\n                fe = feature_extractor.casefold()\n                if fe == 'tsne_exact'.casefold():\n                    fe = TSNE(n_components=2,\n                              metric='cosine',\n                              perplexity=perplexity,\n                              n_iter=1000,\n                              method='exact',\n                              random_state=self.random_state,\n                              n_jobs=self.n_jobs)\n                elif fe == 'tsne'.casefold():\n                    fe = TSNE(n_components=2,\n                              metric='cosine',\n                              perplexity=perplexity,\n                              n_iter=1000,\n                              method='barnes_hut',\n                              random_state=self.random_state,\n                              n_jobs=self.n_jobs)\n                elif fe == 'pca'.casefold():\n                    fe = PCA(n_components=2, random_state=self.random_state)\n                elif fe == 'kpca'.casefold():\n                    fe = KernelPCA(n_components=2,\n                                   kernel='rbf',\n                                   random_state=self.random_state,\n                                   n_jobs=self.n_jobs)\n                else:\n                    raise ValueError((\"Feature extraction method '{}' not accepted\"\n                                      ).format(feature_extractor))\n                self._fe = fe\n            elif hasattr(feature_extractor, 'fit_transform') and \\\n                    inspect.ismethod(feature_extractor.fit_transform):\n                self._fe = feature_extractor\n            else:\n                raise TypeError('Parameter feature_extractor is not a '\n                                'string nor has method \"fit_transform\"')\n\n            if isinstance(pixels, int):\n                pixels = (pixels, pixels)\n\n            # The resolution of transformed image\n            self._pixels = pixels\n            self._xrot = None\n\n        def fit(self, X, y=None, plot=False):\n            \"\"\"Train the image transformer from the training set (X)\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                y: Ignored. Present for continuity with scikit-learn\n                plot: boolean of whether to produce a scatter plot showing the\n                    feature reduction, hull points, and minimum bounding rectangle\n\n            Returns:\n                self: object\n            \"\"\"\n            # Transpose to get (n_features, n_samples)\n            X = X.T\n\n            # Perform dimensionality reduction\n            x_new = self._fe.fit_transform(X)\n\n            # Get the convex hull for the points\n            chvertices = ConvexHull(x_new).vertices\n            hull_points = x_new[chvertices]\n\n            # Determine the minimum bounding rectangle\n            mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n\n            # Rotate the matrix\n            # Save the rotated matrix in case user wants to change the pixel size\n            self._xrot = np.dot(mbr_rot, x_new.T).T\n\n            # Determine feature coordinates based on pixel dimension\n            self._calculate_coords()\n\n            # plot rotation diagram if requested\n            if plot is True:\n                # Create subplots\n                fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n                ax[0, 0].scatter(x_new[:, 0],\n                                 x_new[:, 1],\n                                 cmap=plt.cm.get_cmap(\"jet\", 10),\n                                 marker=\"x\",\n                                 alpha=1.0)\n                ax[0, 0].fill(x_new[chvertices, 0],\n                              x_new[chvertices, 1],\n                              edgecolor='r',\n                              fill=False)\n                ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n                plt.gca().set_aspect('equal', adjustable='box')\n                plt.show()\n            return self\n\n        @property\n        def pixels(self):\n            \"\"\"The image matrix dimensions\n\n            Returns:\n                tuple: the image matrix dimensions (height, width)\n\n            \"\"\"\n            return self._pixels\n\n        @pixels.setter\n        def pixels(self, pixels):\n            \"\"\"Set the image matrix dimension\n\n            Args:\n                pixels: int or tuple with the dimensions (height, width)\n                of the image matrix\n\n            \"\"\"\n            if isinstance(pixels, int):\n                pixels = (pixels, pixels)\n            self._pixels = pixels\n            # recalculate coordinates if already fit\n            if hasattr(self, '_coords'):\n                self._calculate_coords()\n\n        def _calculate_coords(self):\n            \"\"\"Calculate the matrix coordinates of each feature based on the\n            pixel dimensions.\n            \"\"\"\n            ax0_coord = np.digitize(self._xrot[:, 0],\n                                    bins=np.linspace(min(self._xrot[:, 0]),\n                                                     max(self._xrot[:, 0]),\n                                                     self._pixels[0])) - 1\n            ax1_coord = np.digitize(self._xrot[:, 1],\n                                    bins=np.linspace(min(self._xrot[:, 1]),\n                                                     max(self._xrot[:, 1]),\n                                                     self._pixels[1])) - 1\n            self._coords = np.stack((ax0_coord, ax1_coord))\n\n        def transform(self, X, empty_value=0):\n            \"\"\"Transform the input matrix into image matrices\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                    where n_features matches the training set.\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n            # Group by location (x1, y1) of each feature\n            # Tranpose to get (n_features, n_samples)\n            img_coords = pd.DataFrame(np.vstack(\n                (self._coords, X.clip(0, 1))).T).groupby(\n                    [0, 1],  # (x1, y1)\n                    as_index=False).mean()\n\n            img_matrices = []\n            blank_mat = np.zeros(self._pixels)\n            if empty_value != 0:\n                blank_mat[:] = empty_value\n            for z in range(2, img_coords.shape[1]):\n                img_matrix = blank_mat.copy()\n                img_matrix[img_coords[0].astype(int),\n                           img_coords[1].astype(int)] = img_coords[z]\n                img_matrices.append(img_matrix)\n                \n            img_matrices = np.array([self._mat_to_rgb(m) for m in img_matrices])\n                \n            return img_matrices\n        \n        def transform_3d(self, X, empty_value=0):\n            \"\"\"Transform the input matrix into image matrices\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                    where n_features matches the training set.\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n\n            # Group by location (x1, y1) of each feature\n            # Tranpose to get (n_features, n_samples)\n            img_coords = pd.DataFrame(np.vstack(\n                (self._coords, X.clip(0, 1))).T).groupby(\n                    [0, 1],  # (x1, y1)\n                    as_index=False)\n            avg_img_coords = img_coords.mean()\n            min_img_coords = img_coords.min()\n            max_img_coords = img_coords.max()\n\n            img_matrices = []\n            blank_mat = np.zeros((3, self._pixels[0], self._pixels[1]))\n            if empty_value != 0:\n                blank_mat[:, :, :] = empty_value\n            for z in range(2, avg_img_coords.shape[1]):\n                img_matrix = blank_mat.copy()\n                img_matrix[0, avg_img_coords[0].astype(int),\n                           avg_img_coords[1].astype(int)] = avg_img_coords[z]\n                img_matrix[1, min_img_coords[0].astype(int),\n                           min_img_coords[1].astype(int)] = min_img_coords[z]\n                img_matrix[2, max_img_coords[0].astype(int),\n                           max_img_coords[1].astype(int)] = max_img_coords[z]\n                img_matrices.append(img_matrix)\n\n            return img_matrices\n\n        def fit_transform(self, X, empty_value=0):\n            \"\"\"Train the image transformer from the training set (X) and return\n            the transformed data.\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n            self.fit(X)\n            return self.transform(X, empty_value=empty_value)\n        \n        def fit_transform_3d(self, X, empty_value=0):\n            \"\"\"Train the image transformer from the training set (X) and return\n            the transformed data.\n\n            Args:\n                X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                empty_value: numeric value to fill elements where no features are\n                    mapped. Default = 0 (although it was 1 in the paper).\n\n            Returns:\n                A list of n_samples numpy matrices of dimensions set by\n                the pixel parameter\n            \"\"\"\n            self.fit(X)\n            return self.transform_3d(X, empty_value=empty_value)\n\n        def feature_density_matrix(self):\n            \"\"\"Generate image matrix with feature counts per pixel\n\n            Returns:\n                img_matrix (ndarray): matrix with feature counts per pixel\n            \"\"\"\n            fdmat = np.zeros(self._pixels)\n            # Group by location (x1, y1) of each feature\n            # Tranpose to get (n_features, n_samples)\n            coord_cnt = (\n                pd.DataFrame(self._coords.T).assign(count=1).groupby(\n                    [0, 1],  # (x1, y1)\n                    as_index=False).count())\n            fdmat[coord_cnt[0].astype(int),\n                  coord_cnt[1].astype(int)] = coord_cnt['count']\n            return fdmat\n\n        @staticmethod\n        def _minimum_bounding_rectangle(hull_points):\n            \"\"\"Find the smallest bounding rectangle for a set of points.\n\n            Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n            Returns a set of points representing the corners of the bounding box.\n\n            Args:\n                hull_points : an nx2 matrix of hull coordinates\n\n            Returns:\n                (tuple): tuple containing\n                    coords (ndarray): coordinates of the corners of the rectangle\n                    rotmat (ndarray): rotation matrix to align edges of rectangle\n                        to x and y\n            \"\"\"\n\n            pi2 = np.pi / 2.\n\n            # Calculate edge angles\n            edges = hull_points[1:] - hull_points[:-1]\n            angles = np.arctan2(edges[:, 1], edges[:, 0])\n            angles = np.abs(np.mod(angles, pi2))\n            angles = np.unique(angles)\n\n            # Find rotation matrices\n            rotations = np.vstack([\n                np.cos(angles),\n                np.cos(angles - pi2),\n                np.cos(angles + pi2),\n                np.cos(angles)\n            ]).T\n            rotations = rotations.reshape((-1, 2, 2))\n\n            # Apply rotations to the hull\n            rot_points = np.dot(rotations, hull_points.T)\n\n            # Find the bounding points\n            min_x = np.nanmin(rot_points[:, 0], axis=1)\n            max_x = np.nanmax(rot_points[:, 0], axis=1)\n            min_y = np.nanmin(rot_points[:, 1], axis=1)\n            max_y = np.nanmax(rot_points[:, 1], axis=1)\n\n            # Find the box with the best area\n            areas = (max_x - min_x) * (max_y - min_y)\n            best_idx = np.argmin(areas)\n\n            # Return the best box\n            x1 = max_x[best_idx]\n            x2 = min_x[best_idx]\n            y1 = max_y[best_idx]\n            y2 = min_y[best_idx]\n            rotmat = rotations[best_idx]\n\n            # Generate coordinates\n            coords = np.zeros((4, 2))\n            coords[0] = np.dot([x1, y2], rotmat)\n            coords[1] = np.dot([x2, y2], rotmat)\n            coords[2] = np.dot([x2, y1], rotmat)\n            coords[3] = np.dot([x1, y1], rotmat)\n\n            return coords, rotmat\n        \n        @staticmethod\n        def _mat_to_rgb(mat):\n            \"\"\"Convert image matrix to numpy rgb format\n            Args:\n                mat: {array-like} (M, N)\n            Returns:\n                An numpy.ndarry (M, N, 3) with orignal values repeated across\n                RGB channels.\n            \"\"\"\n            return np.repeat(mat[:, :, np.newaxis], 3, axis=2)\n\n    class TrainDataset:\n#         def __init__(self, features, targets):\n#             self.features = features\n#             self.targets = targets\n\n#         def __len__(self):\n#             return (self.features.shape[0])\n\n#         def __getitem__(self, idx):\n#             dct = {\n#                 'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n#                 'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n#             }\n#             return dct\n        \n        ####### Non-swap ######\n        def __init__(self, features, labels, transformer):\n            self.features = features\n            self.labels = labels\n            self.transformer = transformer\n\n        def __getitem__(self, index):\n            normalized = self.features[index, :]\n            normalized = np.expand_dims(normalized, axis=0)\n\n            # Note: we are setting empty_value=1 to follow the setup in the paper\n            image = self.transformer.transform(normalized, empty_value=1)[0]\n            \n            preprocess = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n\n            return {\"x\": preprocess(image).type(torch.float), \"y\": torch.tensor(self.labels[index, :],dtype=torch.float)}\n        \n        \n        def __len__(self):\n            return self.features.shape[0]\n\n    class TestDataset:\n#         def __init__(self, features):\n#             self.features = features\n\n#         def __len__(self):\n#             return (self.features.shape[0])\n\n#         def __getitem__(self, idx):\n#             dct = {\n#                 'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n#             }\n#             return dct\n\n        def __init__(self, features, transformer):\n            self.features = features\n            self.transformer = transformer\n\n        def __getitem__(self, index):\n            normalized = self.features[index, :]\n            normalized = np.expand_dims(normalized, axis=0)\n\n            # Note: we are setting empty_value=1 to follow the setup in the paper\n            image = self.transformer.transform(normalized, empty_value=1)[0]\n            \n            preprocess = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n            \n            return  {\"x\": preprocess(image).type(torch.float)}\n\n        def __len__(self):\n            return self.features.shape[0]\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n\n        x_train, y_train,y_train_ns = train_df[feature_cols].values, train_df[target_cols].values,train_df[target_nonsc_cols2].values\n        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols].values, valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n        x_test = test_[feature_cols].values\n        \n        def load_pickle(model_output_folder, seed_i, fold_i, name):\n            return load(open(f\"{model_output_folder}/seed{seed_i}_fold{fold_i}_{name}.pkl\", 'rb'))\n        \n        # LogScaler (Norm-2 Normalization)\n#         print(\"Running norm-2 normalization ......\")\n        \n        scaler = load_pickle(mod_path1 + \"Resnet_deepinsight\", seed, fold, \"log-scaler\")\n        x_test = scaler.transform(x_test)\n        x_train = scaler.transform(x_train)\n        x_valid = scaler.transform(x_valid)\n        \n        # Load DeepInsight Feature Map\n        transformer = load_pickle(mod_path1 + \"Resnet_deepinsight\", seed, fold,\n                                  \"deepinsight-transform\")\n        \n        \n        train_dataset = TrainDataset(x_train, y_train, transformer)\n        valid_dataset = TrainDataset(x_valid, y_valid, transformer)\n        \n#         train_dataset = TrainDataset(x_train, y_train)\n#         valid_dataset = TrainDataset(x_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        oof = np.zeros((len(train), len(target_cols)))\n        best_loss = np.inf\n\n        mod_name = mod_path1 + f\"FOLD_mod11_{seed}_{fold}_.pth\"\n        \n        model.fc = nn.Sequential(nn.Linear(2048, 512),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.2),\n                                 nn.Linear(512, num_targets))\n        \n        model.load_state_dict(torch.load(mod_name))\n        model.to(DEVICE)\n        \n        oof[val_idx] = inference_fn(model, validloader, DEVICE)\n        \n        #--------------------- PREDICTION---------------------\n        testdataset = TestDataset(x_test, transformer)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), len(target_cols)))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            print('seed:',seed,'fold:',fold)\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n    \n    oof_tmp = dp(oof)\n    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n\nprint(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n\ntrain0[target_cols] = oof\ntest[target_cols] = predictions\n\nsub2 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n### mod1 ###\ntrain0_1 = train0.copy()\nsub_1 = sub2.copy()\n\npd.DataFrame(sc_dic,index=['sc']).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final submission\nsub2.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(input_dir+'sample_submission.csv')\nsubmission.iloc[:, 1:] = 0\nsubmission.iloc[:, 1:] = sub1.iloc[:,1:]*0.9 + sub2.iloc[:,1:]*0.1 \nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}