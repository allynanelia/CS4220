{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-18T12:06:32.162946Z",
     "iopub.status.busy": "2021-04-18T12:06:32.162181Z",
     "iopub.status.idle": "2021-04-18T12:06:32.269734Z",
     "shell.execute_reply": "2021-04-18T12:06:32.269010Z"
    },
    "papermill": {
     "duration": 0.123445,
     "end_time": "2021-04-18T12:06:32.269960",
     "exception": false,
     "start_time": "2021-04-18T12:06:32.146515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_0_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_6_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_12_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_6_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_2_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_4_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_13_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_9_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_8_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_1_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_9_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_4_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_4_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_0_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_12_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_11_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_8_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_15_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_13_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_14_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_15_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_9_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_7_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_7_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_7_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_8_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_5_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_1_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_11_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_3_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_3_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_12_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_2_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_5_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_6_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_10_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_5_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_4_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_12_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_0_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_15_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_1_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_6_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_9_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_13_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_7_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/train_pred.csv\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_14_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_2_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_2_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_6_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_13_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_11_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/submission.csv\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_5_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_12_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_14_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_11_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_10_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_8_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_15_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_1_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_8_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_9_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_5_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_15_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_11_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_10_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_3_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_14_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_4_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_2_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_3_3_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_10_0_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_0_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_7_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_1_4_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_14_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_3_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_10_1_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_0_2_.pth\n",
      "/kaggle/input/og-1dcnn-training-final/FOLD_mod11_13_1_.pth\n",
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.travis.yml\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.cfg\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/LICENSE\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.gitignore\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/README.md\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/test_ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-04-18T12:06:32.299609Z",
     "iopub.status.busy": "2021-04-18T12:06:32.298834Z",
     "iopub.status.idle": "2021-04-18T12:06:34.726271Z",
     "shell.execute_reply": "2021-04-18T12:06:34.725627Z"
    },
    "papermill": {
     "duration": 2.444895,
     "end_time": "2021-04-18T12:06:34.726421",
     "exception": false,
     "start_time": "2021-04-18T12:06:32.281526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy as dp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "# # TabNet\n",
    "# !pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n",
    "\n",
    "# # Tabnet \n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from pytorch_tabnet.metrics import Metric\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# feature transformation - fit\n",
    "def norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
    "    ss_1_dic = {'zsco':StandardScaler(),\n",
    "                'mima':MinMaxScaler(),\n",
    "                'maxb':MaxAbsScaler(), \n",
    "                'robu':RobustScaler(),\n",
    "                'norm':Normalizer(), \n",
    "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
    "                'powe':PowerTransformer()}\n",
    "    ss_1 = ss_1_dic[sc_name]\n",
    "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    if saveM == False:\n",
    "        return(df_2)\n",
    "    else:\n",
    "        return(df_2,ss_1)\n",
    "\n",
    "# feature transformation - trans\n",
    "def norm_tra(df_1,ss_x):\n",
    "    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    return(df_2)\n",
    "\n",
    "# frequency \n",
    "def f_table(list1):\n",
    "    table_dic = {}\n",
    "    for i in list1:\n",
    "        if i not in table_dic.keys():\n",
    "            table_dic[i] = 1\n",
    "        else:\n",
    "            table_dic[i] += 1\n",
    "    return(table_dic)\n",
    "\n",
    "# seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "# input data dir\n",
    "input_dir = '../input/lish-moa/'\n",
    "# upload model dataset from training kernel outputs\n",
    "mod_path1 = '../input/og-1dcnn-training-final/'                 # 1D-CNN\n",
    "# mod_path2 = '../input/fork-of-ble-w1-6-2-kd-tab1-wgt1-pa1-4/' # TabNet\n",
    "# mod_path3 = '../input/ble2-nntrans-tr/'                       # DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009985,
     "end_time": "2021-04-18T12:06:34.748592",
     "exception": false,
     "start_time": "2021-04-18T12:06:34.738607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# mod1: 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:06:34.809936Z",
     "iopub.status.busy": "2021-04-18T12:06:34.782244Z",
     "iopub.status.idle": "2021-04-18T12:46:22.239582Z",
     "shell.execute_reply": "2021-04-18T12:46:22.239974Z"
    },
    "papermill": {
     "duration": 2387.481675,
     "end_time": "2021-04-18T12:46:22.240154",
     "exception": false,
     "start_time": "2021-04-18T12:06:34.758479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "gene 772\n",
      "cell 100\n",
      "0.01672003771081815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.016757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.016740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.016736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.016722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.016722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.016721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.016720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sc\n",
       "0   0.017204\n",
       "1   0.016958\n",
       "2   0.016880\n",
       "3   0.016831\n",
       "4   0.016801\n",
       "5   0.016777\n",
       "6   0.016762\n",
       "7   0.016757\n",
       "8   0.016749\n",
       "9   0.016740\n",
       "10  0.016736\n",
       "11  0.016729\n",
       "12  0.016722\n",
       "13  0.016722\n",
       "14  0.016721\n",
       "15  0.016720"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "sc_dic = {}\n",
    "feat_dic = {}\n",
    "train_features = pd.read_csv(input_dir+'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv(input_dir+'test_features.csv')\n",
    "sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n",
    "train_drug = pd.read_csv(input_dir+'train_drug.csv')\n",
    "\n",
    "target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "# non-score targets highly correlated with scored targets will be used in pretrain\n",
    "nonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\n",
    "tmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\n",
    "mat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n",
    "                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\n",
    "mat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\n",
    "mat_cor2.index = target_nonsc_cols\n",
    "mat_cor2.columns = target_cols\n",
    "mat_cor2 = mat_cor2.dropna()\n",
    "mat_cor2_max = mat_cor2.abs().max(axis = 1)\n",
    "\n",
    "q_n_cut = 0.9\n",
    "target_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\n",
    "print(len(target_nonsc_cols2))\n",
    "\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feat_dic['gene'] = GENES\n",
    "feat_dic['cell'] = CELLS\n",
    "\n",
    "# sample normalization \n",
    "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
    "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "# remove ctr \n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n",
    "\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[['sig_id']+target_cols]\n",
    "target_ns = train[['sig_id']+target_nonsc_cols2]\n",
    "\n",
    "train0 = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "# drug ids\n",
    "tar_sig = target['sig_id'].tolist()\n",
    "train_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\n",
    "target = target.merge(train_drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_drug.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 19].index\n",
    "vc2 = vc.loc[vc > 19].index\n",
    "\n",
    "feature_cols = []\n",
    "for key_i in feat_dic.keys():\n",
    "    value_i = feat_dic[key_i]\n",
    "    print(key_i,len(value_i))\n",
    "    feature_cols += value_i\n",
    "feature_cols0 = dp(feature_cols)\n",
    "    \n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "for seed in SEED:\n",
    "    seed_everything(seed=seed)\n",
    "    folds = train0.copy()\n",
    "    feature_cols = dp(feature_cols0)\n",
    "    \n",
    "    # Kfold - leave drug out\n",
    "    target2 = target.copy()\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    target2['kfold'] = target2.drug_id.map(dct1)\n",
    "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
    "    target2.kfold = target2.kfold.astype(int)\n",
    "\n",
    "    folds['kfold'] = target2['kfold'].copy()\n",
    "\n",
    "    train = folds.copy()\n",
    "    test_ = test.copy()\n",
    "\n",
    "    # HyperParameters\n",
    "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    EPOCHS = 25\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    NFOLDS = 5\n",
    "    EARLY_STOPPING_STEPS = 10\n",
    "    EARLY_STOP = False\n",
    "\n",
    "    n_comp1 = 50\n",
    "    n_comp2 = 15\n",
    "\n",
    "    num_features=len(feature_cols) + n_comp1 + n_comp2\n",
    "    num_targets=len(target_cols)\n",
    "    num_targets_0=len(target_nonsc_cols2)\n",
    "    hidden_size=4096\n",
    "\n",
    "    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
    "    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "    tar_weight0_min = dp(np.min(tar_weight0))\n",
    "    tar_weight = tar_weight0_min/tar_weight0\n",
    "    np.mean(tar_weight)\n",
    "    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
    "    \n",
    "    class SmoothBCEwLogits(_WeightedLoss):\n",
    "        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "            super().__init__(weight=weight, reduction=reduction)\n",
    "            self.smoothing = smoothing\n",
    "            self.weight = weight\n",
    "            self.reduction = reduction\n",
    "\n",
    "        @staticmethod\n",
    "        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "            assert 0 <= smoothing < 1\n",
    "            with torch.no_grad():\n",
    "                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            return targets\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                self.smoothing)\n",
    "            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = pos_weight)\n",
    "\n",
    "            if  self.reduction == 'sum':\n",
    "                loss = loss.sum()\n",
    "            elif  self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "\n",
    "            return loss\n",
    "\n",
    "    class TrainDataset:\n",
    "        def __init__(self, features, targets):\n",
    "            self.features = features\n",
    "            self.targets = targets\n",
    "\n",
    "        def __len__(self):\n",
    "            return (self.features.shape[0])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dct = {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "            }\n",
    "            return dct\n",
    "\n",
    "    class TestDataset:\n",
    "        def __init__(self, features):\n",
    "            self.features = features\n",
    "\n",
    "        def __len__(self):\n",
    "            return (self.features.shape[0])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dct = {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "            }\n",
    "            return dct\n",
    "\n",
    "\n",
    "    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "        model.train()\n",
    "        final_loss = 0\n",
    "\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            final_loss += loss.item()\n",
    "\n",
    "        final_loss /= len(dataloader)\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "\n",
    "    def valid_fn(model, loss_fn, dataloader, device):\n",
    "        model.eval()\n",
    "        final_loss = 0\n",
    "        valid_preds = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            final_loss += loss.item()\n",
    "            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        final_loss /= len(dataloader)\n",
    "        valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "        return final_loss, valid_preds\n",
    "\n",
    "    def inference_fn(model, dataloader, device):\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs = data['x'].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, num_features, num_targets, hidden_size):\n",
    "            super(Model, self).__init__()\n",
    "            cha_1 = 256\n",
    "            cha_2 = 512\n",
    "            cha_3 = 512\n",
    "\n",
    "            cha_1_reshape = int(hidden_size/cha_1)\n",
    "            cha_po_1 = int(hidden_size/cha_1/2)\n",
    "            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "            self.cha_1 = cha_1\n",
    "            self.cha_2 = cha_2\n",
    "            self.cha_3 = cha_3\n",
    "            self.cha_1_reshape = cha_1_reshape\n",
    "            self.cha_po_1 = cha_po_1\n",
    "            self.cha_po_2 = cha_po_2\n",
    "\n",
    "            self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "            self.dropout1 = nn.Dropout(0.1)\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "            self.dropout_c1 = nn.Dropout(0.1)\n",
    "            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2 = nn.Dropout(0.1)\n",
    "            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            self.flt = nn.Flatten()\n",
    "\n",
    "            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "            self.dropout3 = nn.Dropout(0.2)\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            x = self.batch_norm1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "            x = x.reshape(x.shape[0],self.cha_1,\n",
    "                          self.cha_1_reshape)\n",
    "\n",
    "            x = self.batch_norm_c1(x)\n",
    "            x = self.dropout_c1(x)\n",
    "            x = F.relu(self.conv1(x))\n",
    "\n",
    "            x = self.ave_po_c1(x)\n",
    "\n",
    "            x = self.batch_norm_c2(x)\n",
    "            x = self.dropout_c2(x)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_s = x\n",
    "\n",
    "            x = self.batch_norm_c2_1(x)\n",
    "            x = self.dropout_c2_1(x)\n",
    "            x = F.relu(self.conv2_1(x))\n",
    "\n",
    "            x = self.batch_norm_c2_2(x)\n",
    "            x = self.dropout_c2_2(x)\n",
    "            x = F.relu(self.conv2_2(x))\n",
    "            x =  x * x_s\n",
    "\n",
    "            x = self.max_po_c2(x)\n",
    "\n",
    "            x = self.flt(x)\n",
    "\n",
    "            x = self.batch_norm3(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.dense3(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def run_training(fold, seed):\n",
    "\n",
    "        seed_everything(seed)\n",
    "\n",
    "        trn_idx = train[train['kfold'] != fold].index\n",
    "        val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
    "        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
    "\n",
    "        x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n",
    "        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n",
    "        x_test = test_[feature_cols]\n",
    "\n",
    "        #------------ norm --------------\n",
    "        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
    "        col_num.sort()\n",
    "        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
    "        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
    "        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
    "\n",
    "        #------------ pca --------------\n",
    "        def pca_pre(tr,va,te,\n",
    "                    n_comp,feat_raw,feat_new):\n",
    "            pca = PCA(n_components=n_comp, random_state=42)\n",
    "            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
    "            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
    "            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
    "            return(tr2,va2,te2)\n",
    "\n",
    "\n",
    "        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
    "        feat_dic['pca_g'] = pca_feat_g\n",
    "        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
    "\n",
    "        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
    "        feat_dic['pca_c'] = pca_feat_g\n",
    "        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
    "\n",
    "        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
    "        \n",
    "        train_dataset = TrainDataset(x_train, y_train)\n",
    "        valid_dataset = TrainDataset(x_valid, y_valid)\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n",
    "        loss_va = nn.BCEWithLogitsLoss()    \n",
    "\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        best_loss = np.inf\n",
    "\n",
    "        mod_name = mod_path1 + f\"FOLD_mod11_{seed}_{fold}_.pth\"\n",
    "        \n",
    "        model.load_state_dict(torch.load(mod_name, map_location=torch.device('cpu')))\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        oof[val_idx] = inference_fn(model, validloader, DEVICE)\n",
    "        \n",
    "        #--------------------- PREDICTION---------------------\n",
    "        testdataset = TestDataset(x_test)\n",
    "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        predictions = np.zeros((len(test_), len(target_cols)))\n",
    "        predictions = inference_fn(model, testloader, DEVICE)\n",
    "        return oof, predictions\n",
    "\n",
    "    def run_k_fold(NFOLDS, seed):\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "        for fold in range(NFOLDS):\n",
    "            oof_, pred_ = run_training(fold, seed)\n",
    "\n",
    "            predictions += pred_ / NFOLDS\n",
    "            oof += oof_\n",
    "\n",
    "        return oof, predictions\n",
    "\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    oof_tmp = dp(oof)\n",
    "    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
    "    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n",
    "\n",
    "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
    "\n",
    "train0[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "sub1 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "### mod1 ###\n",
    "train0_1 = train0.copy()\n",
    "sub_1 = sub1.copy()\n",
    "\n",
    "pd.DataFrame(sc_dic,index=['sc']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:22.269920Z",
     "iopub.status.busy": "2021-04-18T12:46:22.269407Z",
     "iopub.status.idle": "2021-04-18T12:46:27.030819Z",
     "shell.execute_reply": "2021-04-18T12:46:27.029831Z"
    },
    "papermill": {
     "duration": 4.778976,
     "end_time": "2021-04-18T12:46:27.030958",
     "exception": false,
     "start_time": "2021-04-18T12:46:22.251982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy as dp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:27.060233Z",
     "iopub.status.busy": "2021-04-18T12:46:27.058491Z",
     "iopub.status.idle": "2021-04-18T12:46:27.060824Z",
     "shell.execute_reply": "2021-04-18T12:46:27.061214Z"
    },
    "papermill": {
     "duration": 0.018923,
     "end_time": "2021-04-18T12:46:27.061358",
     "exception": false,
     "start_time": "2021-04-18T12:46:27.042435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed) \n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:27.089852Z",
     "iopub.status.busy": "2021-04-18T12:46:27.089078Z",
     "iopub.status.idle": "2021-04-18T12:46:30.617109Z",
     "shell.execute_reply": "2021-04-18T12:46:30.616609Z"
    },
    "papermill": {
     "duration": 3.544695,
     "end_time": "2021-04-18T12:46:30.617230",
     "exception": false,
     "start_time": "2021-04-18T12:46:27.072535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = '../input/lish-moa/'\n",
    "train_features = pd.read_csv(input_dir+'train_features.csv')\n",
    "train_targets = pd.read_csv(input_dir+'train_targets_scored.csv')\n",
    "test_features = pd.read_csv(input_dir+'test_features.csv')\n",
    "train_drug = pd.read_csv(input_dir+'train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:30.646841Z",
     "iopub.status.busy": "2021-04-18T12:46:30.646066Z",
     "iopub.status.idle": "2021-04-18T12:46:30.854496Z",
     "shell.execute_reply": "2021-04-18T12:46:30.853997Z"
    },
    "papermill": {
     "duration": 0.225343,
     "end_time": "2021-04-18T12:46:30.854624",
     "exception": false,
     "start_time": "2021-04-18T12:46:30.629281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert categorical variable into dummy/indicator variables\n",
    "train_features = pd.get_dummies(train_features, columns=['cp_type', 'cp_dose'])\n",
    "test_features = pd.get_dummies(test_features, columns=['cp_type', 'cp_dose'])\n",
    "\n",
    "# Remove sig_id\n",
    "all_dfs = (train_features, train_targets, test_features)\n",
    "\n",
    "for df in all_dfs:\n",
    "    if 'sig_id' in df.columns:\n",
    "        df.drop('sig_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:30.883998Z",
     "iopub.status.busy": "2021-04-18T12:46:30.882874Z",
     "iopub.status.idle": "2021-04-18T12:46:31.018104Z",
     "shell.execute_reply": "2021-04-18T12:46:31.017617Z"
    },
    "papermill": {
     "duration": 0.151836,
     "end_time": "2021-04-18T12:46:31.018240",
     "exception": false,
     "start_time": "2021-04-18T12:46:30.866404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Appending new components to existing features\n",
    "pca_train = train_features.copy()\n",
    "# pca_train = pd.concat([pca_train, pca_gene_train, pca_cell_train], axis=1)\n",
    "\n",
    "pca_test = test_features.copy()\n",
    "\n",
    "### MODIFY THIS FOR DIFFERENT DATASETS ###\n",
    "\n",
    "THE_TRAIN = pca_train\n",
    "THE_TEST = pca_test\n",
    "\n",
    "##########################################\n",
    "\n",
    "pred_val = np.zeros((THE_TRAIN.shape[0], 206))\n",
    "pred_test = np.zeros((THE_TEST.shape[0], 206))\n",
    "\n",
    "features = THE_TRAIN.values\n",
    "targets = train_targets.values\n",
    "tests = THE_TEST.values\n",
    "\n",
    "validation_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:31.210507Z",
     "iopub.status.busy": "2021-04-18T12:46:31.209845Z",
     "iopub.status.idle": "2021-04-18T12:46:31.213292Z",
     "shell.execute_reply": "2021-04-18T12:46:31.212842Z"
    },
    "papermill": {
     "duration": 0.183255,
     "end_time": "2021-04-18T12:46:31.213408",
     "exception": false,
     "start_time": "2021-04-18T12:46:31.030153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:31.251154Z",
     "iopub.status.busy": "2021-04-18T12:46:31.250026Z",
     "iopub.status.idle": "2021-04-18T12:46:31.252490Z",
     "shell.execute_reply": "2021-04-18T12:46:31.252900Z"
    },
    "papermill": {
     "duration": 0.027674,
     "end_time": "2021-04-18T12:46:31.253023",
     "exception": false,
     "start_time": "2021-04-18T12:46:31.225349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(train_df, *units):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Input(shape=(train_df.shape[1],)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    for u in units[:-1]:\n",
    "        model.add(tf.keras.layers.Dense(units=u, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=units[-1], activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run(features, targets, tests, pred, pe, n_split=5):\n",
    "    kfoldnumber = 0\n",
    "\n",
    "    for train_index, validation_index in KFold(n_split).split(features):\n",
    "        kfoldnumber += 1\n",
    "        print(f'{\"#\" * 30} Fold number {kfoldnumber} {\"#\" * 30}')\n",
    "\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           factor=0.1, patience=3, verbose=0,\n",
    "                                           epsilon=1e-4, mode='min')\n",
    "        \n",
    "        nn_layers = (512, 1024, 206)\n",
    "        model = get_model(features, *nn_layers)\n",
    "        #model.summary()\n",
    "\n",
    "        model.fit(features[train_index],\n",
    "                  targets[train_index],\n",
    "                  batch_size=128,\n",
    "                  epochs=35,\n",
    "                  validation_data=(features[validation_index], targets[validation_index]),\n",
    "                  verbose=0,\n",
    "                  callbacks=[reduce_lr_loss])\n",
    "\n",
    "        print()\n",
    "        print('train loss:\\t', model.evaluate(features[train_index], targets[train_index],\n",
    "                                     verbose=0, batch_size=128))\n",
    "        \n",
    "        validate_score = model.evaluate(features[validation_index],\n",
    "                                         targets[validation_index],\n",
    "                                        verbose=0, batch_size=128)\n",
    "        print('validate loss:\\t', validate_score)\n",
    "        validation_scores.append(validate_score)\n",
    "        \n",
    "\n",
    "        print()\n",
    "        print('predict validation...')\n",
    "\n",
    "        pred[validation_index] = model.predict(features[validation_index],\n",
    "                                              verbose=0, batch_size=128)\n",
    "\n",
    "        print('predict test...')\n",
    "\n",
    "        pe += model.predict(tests, verbose=0, batch_size=128) / n_split\n",
    "        print()\n",
    "        \n",
    "        del model    \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print('###########################################################################\\n\\nFIN')\n",
    "    \n",
    "    return pred, pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:46:31.280764Z",
     "iopub.status.busy": "2021-04-18T12:46:31.280032Z",
     "iopub.status.idle": "2021-04-18T12:48:35.262966Z",
     "shell.execute_reply": "2021-04-18T12:48:35.263396Z"
    },
    "papermill": {
     "duration": 123.998865,
     "end_time": "2021-04-18T12:48:35.263562",
     "exception": false,
     "start_time": "2021-04-18T12:46:31.264697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## Fold number 1 ##############################\n",
      "\n",
      "train loss:\t 0.011809146031737328\n",
      "validate loss:\t 0.015032604336738586\n",
      "\n",
      "predict validation...\n",
      "predict test...\n",
      "\n",
      "############################## Fold number 2 ##############################\n",
      "\n",
      "train loss:\t 0.013591034337878227\n",
      "validate loss:\t 0.015552771277725697\n",
      "\n",
      "predict validation...\n",
      "predict test...\n",
      "\n",
      "############################## Fold number 3 ##############################\n",
      "\n",
      "train loss:\t 0.01152229867875576\n",
      "validate loss:\t 0.014943710528314114\n",
      "\n",
      "predict validation...\n",
      "predict test...\n",
      "\n",
      "############################## Fold number 4 ##############################\n",
      "\n",
      "train loss:\t 0.01023511029779911\n",
      "validate loss:\t 0.015296460129320621\n",
      "\n",
      "predict validation...\n",
      "predict test...\n",
      "\n",
      "############################## Fold number 5 ##############################\n",
      "\n",
      "train loss:\t 0.013228744268417358\n",
      "validate loss:\t 0.015688350424170494\n",
      "\n",
      "predict validation...\n",
      "predict test...\n",
      "\n",
      "###########################################################################\n",
      "\n",
      "FIN\n",
      "0.015302779339253902\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "pred_val, pred_test = run(features, targets, tests, pred_val, pred_test)\n",
    "\n",
    "# Average prediction validation score\n",
    "print(sum(validation_scores)/(len(validation_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:48:35.307758Z",
     "iopub.status.busy": "2021-04-18T12:48:35.306944Z",
     "iopub.status.idle": "2021-04-18T12:48:37.831866Z",
     "shell.execute_reply": "2021-04-18T12:48:37.832289Z"
    },
    "papermill": {
     "duration": 2.55115,
     "end_time": "2021-04-18T12:48:37.832448",
     "exception": false,
     "start_time": "2021-04-18T12:48:35.281298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
      "0     id_0004d9e33                     0.000720                0.001042   \n",
      "1     id_001897cda                     0.000252                0.000497   \n",
      "2     id_002429b5b                     0.000005                0.000003   \n",
      "3     id_00276f245                     0.000224                0.000432   \n",
      "4     id_0027f1083                     0.001015                0.001054   \n",
      "...            ...                          ...                     ...   \n",
      "3977  id_ff7004b87                     0.000190                0.000658   \n",
      "3978  id_ff925dd0d                     0.003963                0.001268   \n",
      "3979  id_ffb710450                     0.000575                0.000542   \n",
      "3980  id_ffbb869f2                     0.000641                0.000787   \n",
      "3981  id_ffd5800b6                     0.000253                0.000507   \n",
      "\n",
      "      acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
      "0           0.001365                        0.016049   \n",
      "1           0.001211                        0.002490   \n",
      "2           0.000040                        0.000289   \n",
      "3           0.001664                        0.008904   \n",
      "4           0.001261                        0.011803   \n",
      "...              ...                             ...   \n",
      "3977        0.001438                        0.003567   \n",
      "3978        0.000585                        0.004271   \n",
      "3979        0.000544                        0.010754   \n",
      "3980        0.000999                        0.017413   \n",
      "3981        0.001396                        0.009898   \n",
      "\n",
      "      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
      "0                              0.028694                        0.004135   \n",
      "1                              0.002895                        0.001535   \n",
      "2                              0.000193                        0.000042   \n",
      "3                              0.007164                        0.003352   \n",
      "4                              0.019390                        0.003956   \n",
      "...                                 ...                             ...   \n",
      "3977                           0.007197                        0.001956   \n",
      "3978                           0.029494                        0.003831   \n",
      "3979                           0.031676                        0.003927   \n",
      "3980                           0.020118                        0.005425   \n",
      "3981                           0.015433                        0.002460   \n",
      "\n",
      "      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
      "0                       0.002227                       0.003702   \n",
      "1                       0.001847                       0.011780   \n",
      "2                       0.000095                       0.000049   \n",
      "3                       0.001713                       0.003180   \n",
      "4                       0.005084                       0.002776   \n",
      "...                          ...                            ...   \n",
      "3977                    0.000918                       0.005934   \n",
      "3978                    0.005516                       0.003152   \n",
      "3979                    0.003430                       0.003888   \n",
      "3980                    0.003424                       0.002971   \n",
      "3981                    0.002036                       0.005110   \n",
      "\n",
      "      adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
      "0                       0.000145  ...                               0.000344   \n",
      "1                       0.004069  ...                               0.000409   \n",
      "2                       0.000009  ...                               0.000004   \n",
      "3                       0.000263  ...                               0.000263   \n",
      "4                       0.000241  ...                               0.000377   \n",
      "...                          ...  ...                                    ...   \n",
      "3977                    0.000483  ...                               0.000285   \n",
      "3978                    0.000825  ...                               0.000181   \n",
      "3979                    0.000174  ...                               0.000190   \n",
      "3980                    0.000332  ...                               0.000222   \n",
      "3981                    0.000148  ...                               0.000299   \n",
      "\n",
      "      trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
      "0         0.000786         0.002264           0.001711   \n",
      "1         0.000737         0.001921           0.000322   \n",
      "2         0.000026         0.000119           0.000037   \n",
      "3         0.001086         0.002950           0.022899   \n",
      "4         0.000617         0.004914           0.004190   \n",
      "...            ...              ...                ...   \n",
      "3977      0.001848         0.001810           0.009538   \n",
      "3978      0.000511         0.002161           0.000477   \n",
      "3979      0.000226         0.001085           0.000380   \n",
      "3980      0.000315         0.002392           0.000506   \n",
      "3981      0.001836         0.001594           0.001746   \n",
      "\n",
      "      tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
      "0                      0.000239                               0.000262   \n",
      "1                      0.013580                               0.000352   \n",
      "2                      0.000209                               0.000003   \n",
      "3                      0.007596                               0.000207   \n",
      "4                      0.000595                               0.000345   \n",
      "...                         ...                                    ...   \n",
      "3977                   0.004451                               0.000370   \n",
      "3978                   0.000894                               0.000342   \n",
      "3979                   0.000852                               0.000228   \n",
      "3980                   0.000897                               0.000258   \n",
      "3981                   0.001409                               0.000255   \n",
      "\n",
      "      vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
      "0            0.000249   0.001348                    0.001670       0.001409  \n",
      "1            0.010318   0.000726                    0.001197       0.001308  \n",
      "2            0.000149   0.000048                    0.000019       0.000026  \n",
      "3            0.002279   0.001558                    0.001037       0.001287  \n",
      "4            0.000801   0.001168                    0.000583       0.001059  \n",
      "...               ...        ...                         ...            ...  \n",
      "3977         0.002959   0.000766                    0.000737       0.000610  \n",
      "3978         0.002269   0.000963                    0.000364       0.001064  \n",
      "3979         0.000490   0.000998                    0.000315       0.000889  \n",
      "3980         0.001420   0.001370                    0.000562       0.003285  \n",
      "3981         0.001535   0.001302                    0.000424       0.000938  \n",
      "\n",
      "[3982 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = pd.read_csv(input_dir + \"train_targets_scored.csv\")\n",
    "columns.drop('sig_id', axis=1, inplace=True)\n",
    "sub2 = pd.DataFrame(data=pred_test, columns=columns.columns)\n",
    "sample = pd.read_csv(input_dir + \"sample_submission.csv\")\n",
    "sub2.insert(0, column='sig_id', value=sample['sig_id'])\n",
    "\n",
    "sub2.to_csv('submission.csv', index=False)\n",
    "print(sub2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019675,
     "end_time": "2021-04-18T12:48:37.874513",
     "exception": false,
     "start_time": "2021-04-18T12:48:37.854838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:48:37.917195Z",
     "iopub.status.busy": "2021-04-18T12:48:37.916628Z",
     "iopub.status.idle": "2021-04-18T12:48:37.920637Z",
     "shell.execute_reply": "2021-04-18T12:48:37.920014Z"
    },
    "papermill": {
     "duration": 0.026559,
     "end_time": "2021-04-18T12:48:37.920778",
     "exception": false,
     "start_time": "2021-04-18T12:48:37.894219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 3 models weighted average\n",
    "# sub = sub_1.copy()\n",
    "# sub[target_cols] = sub_1[target_cols] * 0.65 + sub_2[target_cols] * 0.1 + sub_3[target_cols] * 0.25\n",
    "\n",
    "# final submission\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-18T12:48:37.966574Z",
     "iopub.status.busy": "2021-04-18T12:48:37.965732Z",
     "iopub.status.idle": "2021-04-18T12:48:40.326614Z",
     "shell.execute_reply": "2021-04-18T12:48:40.325662Z"
    },
    "papermill": {
     "duration": 2.386478,
     "end_time": "2021-04-18T12:48:40.326768",
     "exception": false,
     "start_time": "2021-04-18T12:48:37.940290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(input_dir+'sample_submission.csv')\n",
    "submission.iloc[:, 1:] = 0\n",
    "submission.iloc[:, 1:] = sub1.iloc[:,1:]*0.9 + sub2.iloc[:,1:]*0.1 \n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2535.289022,
   "end_time": "2021-04-18T12:48:42.360435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-18T12:06:27.071413",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
